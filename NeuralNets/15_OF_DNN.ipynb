{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== IMPORTING ||| SCRIPT STARTS ||| LOGGING PURPOSE ======\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import double_log\n",
    "def print(*args, **kwargs):\n",
    "    return double_log.print(*args, **kwargs)\n",
    "\n",
    "import keras\n",
    "import pickle\n",
    "import operator\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import plot_conf_matrix as pcm\n",
    "from keras import backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.engine.topology import Layer\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential, Model\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.metrics import confusion_matrix, f1_score\n",
    "from keras.layers import Dense, Dropout, Softmax, BatchNormalization, LeakyReLU, ELU, ThresholdedReLU\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "reload(pcm)\n",
    "\n",
    "print(\"===== IMPORTING ||| SCRIPT STARTS ||| LOGGING PURPOSE ======\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data, Splitting, and Correct Fromating\n",
    "As my noisy dataset, I use MNIST but I flipped the labels with 50% probability to something else with a random distribution. As my ground truth, I use the clean MNIST. You can see the noise distribution in the confusion matrix below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "caucasian     5150\n",
       "eastasian     1549\n",
       "southasian     465\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv('../Data/uniform_table.csv')\n",
    "train = train[train['ethnicity'] != 'hispanic']\n",
    "\n",
    "test = pd.read_csv('../Data/feret_table.csv')\n",
    "test = test[test['race'] != \"other\"]\n",
    "test = test[test['race'] != \"african\"]\n",
    "test = test[test['race'] != \"hispanic\"]\n",
    "test['race'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    5615\n",
       "1    1549\n",
       "Name: eastasian, dtype: int64"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_bin = pd.DataFrame(test)\n",
    "test_bin['eastasian'] = (test_bin['race'] == 'eastasian').astype(int)\n",
    "test_bin['eastasian'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(116009, 128) (116009,) train shape\n",
      "(7164, 128) (7164,) test shape\n"
     ]
    }
   ],
   "source": [
    "batch_size = 128\n",
    "num_classes = 2\n",
    "epochs = 80\n",
    "input_shape = (128,)\n",
    "\n",
    "x_train = train.loc[:,'0':'127'].as_matrix()\n",
    "x_test = test.loc[:,'0':'127'].as_matrix()\n",
    "# y_train = train.loc[:,'ethnicity'].as_matrix()\n",
    "# y_test = test.loc[:,'race'].as_matrix()\n",
    "\n",
    "y_train = train.loc[:,'eastasian'].as_matrix()\n",
    "y_test = test.loc[:,'eastasian'].as_matrix()\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = keras.utils.normalize(x_train)\n",
    "x_test = keras.utils.normalize(x_test)\n",
    "   \n",
    "# 10-fold cross validation\n",
    "folds = list(StratifiedKFold(n_splits=10, shuffle=True, random_state=1).split(x_train, y_train))\n",
    "\n",
    "print(x_train.shape, y_train.shape, 'train shape')\n",
    "print(x_test.shape, y_test.shape,'test shape')\n",
    "\n",
    "# convert class vectors to binary class matrices\n",
    "y_test_clean = y_test\n",
    "encoder = LabelBinarizer()\n",
    "y_train = encoder.fit_transform(y_train)\n",
    "y_test = encoder.transform(y_test)\n",
    "\n",
    "y_test = to_categorical(y_test)\n",
    "y_train = to_categorical(y_train)\n",
    "\n",
    "#if need to figure out relation for the confusion matrix, use inverse_transform\n",
    "cm = np.asarray([[0.82, 0.03, 0.03],[0.02,0.95,0],[0.04,0,0.91]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       ...,\n",
       "       [1., 0.],\n",
       "       [1., 0.],\n",
       "       [1., 0.]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define noise layer and models\n",
    "I defined my custom layer for the noise layer. It initializes its weight matrix either as the true noise distribution (confusion matrix) or as a 10x10 identity matrix depending on the init parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#returns the confusion matrix (the perfectly known noise distribution) in the NoiseLayer initializer\n",
    "def confusion_kernel(shape):\n",
    "    print(\"####################### don not #########\")\n",
    "    return cm\n",
    "def identity_kernel(shape):\n",
    "    return np.rot90(np.eye(num_classes), 3)\n",
    "\n",
    "#noise layer defined according to Keras functional API\n",
    "class NoiseLayer(Layer):\n",
    "    def __init__(self, output_dim, dynamic=True, initializer=identity_kernel, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        self.dynamic = dynamic\n",
    "        self.initializer = initializer\n",
    "        super(NoiseLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # weight matrix\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=input_shape,\n",
    "                                      initializer=self.initializer,\n",
    "                                      trainable=self.dynamic) #change this to false for static weights\n",
    "        super(NoiseLayer, self).build(input_shape)\n",
    "    \n",
    "    #forward pass - vector matrix multiplication of the input and the weights. FIXED for batches\n",
    "    def call(self, x):\n",
    "        return tf.einsum('bn,nm->bn',x, self.kernel)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (1, self.output_dim)\n",
    "    \n",
    "class SoftMaxLayer(Layer):\n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        self.output_dim = output_dim\n",
    "        super(SoftMaxLayer, self).__init__(**kwargs)\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        # weight matrix\n",
    "        self.kernel = self.add_weight(name='kernel', \n",
    "                                      shape=input_shape,\n",
    "                                      initializer='uniform',\n",
    "                                      trainable=False) #change this to false for static weights\n",
    "        super(SoftMaxLayer, self).build(input_shape)\n",
    "    \n",
    "    #forward pass - vector matrix multiplication of the input and the weights. FIXED for batches\n",
    "    def call(self, x):\n",
    "        return tf.nn.softmax(x)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (1, self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for logging in cnn.log\n",
    "print_callback = keras.callbacks.LambdaCallback(\n",
    "        on_epoch_end=lambda epoch,logs: double_log.logger.debug('epoch: '+ str(epoch+1)+ ' logs: '+ str(logs)))\n",
    "\n",
    "\n",
    "activ_list = {\"elu\": ELU(), \"leaky\": LeakyReLU(), \"relu\": keras.layers.Activation(\"relu\"), \"sigmoid\": keras.layers.Activation(\"sigmoid\")}\n",
    "optim_list = {\"adadelta\": keras.optimizers.Adadelta()} \n",
    "matrix_list = {\"identity_kernel\": identity_kernel, \"confusion_kernel\": confusion_kernel}\n",
    "\n",
    "# depth, nodes, initializer, activation, dropout, noise, kernel_init\n",
    "dynamic_params = []\n",
    "for nodes in [32, 64, 128, 248]:\n",
    "    for dropout in [ 0.4, 0.6]:\n",
    "        for depth in [2,3,]:\n",
    "            for activ in [\"leaky\", \"sigmoid\"]:\n",
    "                for init_func in [\"glorot_uniform\"]:\n",
    "                    for optim in [\"adadelta\"]:\n",
    "                        for noise_init in [\"identity_kernel\", \"confusion_kernel\"]:\n",
    "                            dynamic_params.append({'noise': True, 'dynamic':  True, 'nodes': nodes, 'dropout': dropout, 'depth': depth, 'activation': activ, \"initializer\": init_func, \"optimizer\": optim, 'noise_init': noise_init})\n",
    "\n",
    "static_params = []\n",
    "for nodes in [32, 64, 128, 248]:\n",
    "    for dropout in [0.4, 0.6]:\n",
    "        for depth in [3,4]:\n",
    "            for activ in [\"leaky\", \"sigmoid\"]:\n",
    "                for init_func in [\"glorot_uniform\"]:\n",
    "                    for optim in [\"adadelta\"]:\n",
    "                        for noise_init in [\"identity_kernel\", \"confusion_kernel\"]:\n",
    "                            static_params.append({'noise': True, 'dynamic':  False, 'nodes': nodes, 'dropout': dropout, 'depth': depth, 'activation': activ, \"initializer\": init_func, \"optimizer\": optim, 'noise_init': noise_init})\n",
    "\n",
    "default_params = []\n",
    "for nodes in [32, 64, 128, 248]:\n",
    "    for dropout in [0.1,0.3, 0.4, 0.6, 0.8]:\n",
    "        for depth in [1,2,3,4,6]:\n",
    "            for activ in [\"elu\", \"leaky\", \"relu\", \"sigmoid\"]:\n",
    "                for init_func in [\"glorot_uniform\"]:\n",
    "                    for optim in [\"adadelta\"]:\n",
    "                        default_params.append({'noise': False, 'dynamic':  False, 'nodes': nodes, 'dropout': dropout, 'depth': depth, 'activation': activ, \"initializer\": init_func, \"optimizer\": optim})\n",
    "\n",
    "dynamic_params = pd.Series(dynamic_params).sample(1).repeat(5)                        \n",
    "static_params = pd.Series(static_params).sample(1).repeat(5)                        \n",
    "default_params = pd.Series(default_params).sample(1).repeat(5)                        \n",
    "                        \n",
    "def build_model(param):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Dense(param['nodes']+64, kernel_initializer=param['initializer'], input_shape=input_shape))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(activ_list[param['activation']])\n",
    "    model.add(Dropout(param['dropout']))\n",
    "    \n",
    "    for i in range(param['depth']):\n",
    "        model.add(Dense(param['nodes'], kernel_initializer=param['initializer'], input_shape=input_shape))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(activ_list[param['activation']])\n",
    "        model.add(Dropout(param['dropout']))\n",
    "        \n",
    "    model.add(Dense(64, kernel_initializer=param['initializer']))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(num_classes, kernel_initializer=param['initializer'], activation=\"softmax\"))\n",
    "    \n",
    "    # attach noise layer\n",
    "    if (param['noise']):\n",
    "        model.add(NoiseLayer(num_classes, dynamic=param['dynamic'], initializer=matrix_list[param['noise_init']]))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(SoftMaxLayer(num_classes))\n",
    "\n",
    "    model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "                  optimizer=optim_list[param['optimizer']],\n",
    "                  metrics=[keras.metrics.categorical_accuracy])\n",
    "    print(model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(116009, 128)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_params = default_params[0:1]\n",
    "dynamic_params = dynamic_params[0:2]\n",
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEFAULT MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Default - Fold  0  - Param set number  0  ======\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_53 (Dense)             (None, 312)               40248     \n",
      "_________________________________________________________________\n",
      "batch_normalization_35 (Batc (None, 312)               1248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_44 (Dropout)         (None, 312)               0         \n",
      "_________________________________________________________________\n",
      "dense_54 (Dense)             (None, 248)               77624     \n",
      "_________________________________________________________________\n",
      "batch_normalization_36 (Batc (None, 248)               992       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_45 (Dropout)         (None, 248)               0         \n",
      "_________________________________________________________________\n",
      "dense_55 (Dense)             (None, 248)               61752     \n",
      "_________________________________________________________________\n",
      "batch_normalization_37 (Batc (None, 248)               992       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_46 (Dropout)         (None, 248)               0         \n",
      "_________________________________________________________________\n",
      "dense_56 (Dense)             (None, 64)                15936     \n",
      "_________________________________________________________________\n",
      "dropout_47 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_57 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 198,922\n",
      "Trainable params: 197,306\n",
      "Non-trainable params: 1,616\n",
      "_________________________________________________________________\n",
      "None\n",
      "===Default - Fold  1  - Param set number  0  ======\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_58 (Dense)             (None, 312)               40248     \n",
      "_________________________________________________________________\n",
      "batch_normalization_38 (Batc (None, 312)               1248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_48 (Dropout)         (None, 312)               0         \n",
      "_________________________________________________________________\n",
      "dense_59 (Dense)             (None, 248)               77624     \n",
      "_________________________________________________________________\n",
      "batch_normalization_39 (Batc (None, 248)               992       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_49 (Dropout)         (None, 248)               0         \n",
      "_________________________________________________________________\n",
      "dense_60 (Dense)             (None, 248)               61752     \n",
      "_________________________________________________________________\n",
      "batch_normalization_40 (Batc (None, 248)               992       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_50 (Dropout)         (None, 248)               0         \n",
      "_________________________________________________________________\n",
      "dense_61 (Dense)             (None, 64)                15936     \n",
      "_________________________________________________________________\n",
      "dropout_51 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_62 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 198,922\n",
      "Trainable params: 197,306\n",
      "Non-trainable params: 1,616\n",
      "_________________________________________________________________\n",
      "None\n",
      "===Default - Fold  2  - Param set number  0  ======\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_63 (Dense)             (None, 312)               40248     \n",
      "_________________________________________________________________\n",
      "batch_normalization_41 (Batc (None, 312)               1248      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_52 (Dropout)         (None, 312)               0         \n",
      "_________________________________________________________________\n",
      "dense_64 (Dense)             (None, 248)               77624     \n",
      "_________________________________________________________________\n",
      "batch_normalization_42 (Batc (None, 248)               992       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_53 (Dropout)         (None, 248)               0         \n",
      "_________________________________________________________________\n",
      "dense_65 (Dense)             (None, 248)               61752     \n",
      "_________________________________________________________________\n",
      "batch_normalization_43 (Batc (None, 248)               992       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)   multiple                  0         \n",
      "_________________________________________________________________\n",
      "dropout_54 (Dropout)         (None, 248)               0         \n",
      "_________________________________________________________________\n",
      "dense_66 (Dense)             (None, 64)                15936     \n",
      "_________________________________________________________________\n",
      "dropout_55 (Dropout)         (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_67 (Dense)             (None, 2)                 130       \n",
      "=================================================================\n",
      "Total params: 198,922\n",
      "Trainable params: 197,306\n",
      "Non-trainable params: 1,616\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Testing default moel without noise model attached\n",
    "results = []\n",
    "best_score = 0\n",
    "for h, param in enumerate(default_params):\n",
    "    cvscores = []\n",
    "    for j, (train_idx, val_idx) in enumerate(folds[0:3]):\n",
    "        print(\"===Default - Fold \",j,\" - Param set number \",h,\" ======\")\n",
    "        \n",
    "        x_train_cv = x_train[train_idx]\n",
    "        y_train_cv = y_train[train_idx]\n",
    "        x_valid_cv = x_train[val_idx]\n",
    "        y_valid_cv= y_train[val_idx]\n",
    "        \n",
    "        model = build_model(param)\n",
    "        model.fit(x_train_cv, y_train_cv,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  verbose=0,\n",
    "                  validation_data=(x_valid_cv, y_valid_cv),\n",
    "                  callbacks=[print_callback])\n",
    "        \n",
    "        pred = model.predict(x_test, verbose=0)\n",
    "        f1 = f1_score([max(enumerate(i), key=operator.itemgetter(1))[0] for i in pred.tolist()], [max(enumerate(i), key=operator.itemgetter(1))[0] for i in y_test.tolist()], average='micro')\n",
    "        cvscores.append(f1)\n",
    "        if (f1 > best_score):\n",
    "            pred_saved = pred\n",
    "    \n",
    "    results.append({\"parameters\": param, \"f1\": np.mean(cvscores), \"std\": np.std(cvscores)})\n",
    "    \n",
    "results.sort(key=lambda x: x['f1'])\n",
    "with open('../Data/default_results.pickle', 'wb') as fileObj:\n",
    "    pickle.dump(results, fileObj)  \n",
    "    \n",
    "with open('../Data/default_pred.pickle', 'wb') as fileObj:\n",
    "    pickle.dump(pred_saved, fileObj)  \n",
    "\n",
    "print(\"DONE AND SAVED SORTED RESULTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STATIC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Static - Fold  0  - Param set number  0  ======\n",
      "===Static - Fold  1  - Param set number  0  ======\n",
      "===Static - Fold  2  - Param set number  0  ======\n",
      "===Static - Fold  0  - Param set number  1  ======\n",
      "===Static - Fold  1  - Param set number  1  ======\n",
      "===Static - Fold  2  - Param set number  1  ======\n",
      "===Static - Fold  0  - Param set number  2  ======\n",
      "===Static - Fold  1  - Param set number  2  ======\n",
      "===Static - Fold  2  - Param set number  2  ======\n",
      "===Static - Fold  0  - Param set number  3  ======\n",
      "===Static - Fold  1  - Param set number  3  ======\n",
      "===Static - Fold  2  - Param set number  3  ======\n",
      "===Static - Fold  0  - Param set number  4  ======\n",
      "===Static - Fold  1  - Param set number  4  ======\n",
      "===Static - Fold  2  - Param set number  4  ======\n",
      "DONE AND SAVED SORTED RESULTS\n"
     ]
    }
   ],
   "source": [
    "# Testing static model without noise model attached\n",
    "results = []\n",
    "for h, param in enumerate(static_params):\n",
    "    cvscores = []\n",
    "    cvscores2 = []\n",
    "    for j, (train_idx, val_idx) in enumerate(folds[0:3]):\n",
    "        print(\"===Static - Fold \",j,\" - Param set number \",h,\" ======\")\n",
    "        \n",
    "        x_train_cv = x_train[train_idx]\n",
    "        y_train_cv = y_train[train_idx]\n",
    "        x_valid_cv = x_train[val_idx]\n",
    "        y_valid_cv= y_train[val_idx]\n",
    "        \n",
    "        model = build_model(param)\n",
    "        model.fit(x_train_cv, y_train_cv,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  verbose=0,\n",
    "                  validation_data=(x_valid_cv, y_valid_cv),\n",
    "                  callbacks=[print_callback])\n",
    "        \n",
    "        pred = model.predict(x_test, verbose=0)\n",
    "        f1 = f1_score([max(enumerate(i), key=operator.itemgetter(1))[0] for i in pred.tolist()], [max(enumerate(i), key=operator.itemgetter(1))[0] for i in y_test.tolist()], average='micro')\n",
    "        cvscores.append(f1) \n",
    "        \n",
    "        #remove the noise layers to reveal what the base model learnt\n",
    "        model2 = Model(model.input, model.layers[-4].output)\n",
    "        pred2 = model2.predict(x_test, verbose=0)\n",
    "        f12 = f1_score([max(enumerate(i), key=operator.itemgetter(1))[0] for i in pred2.tolist()], [max(enumerate(i), key=operator.itemgetter(1))[0] for i in y_test.tolist()], average='micro')\n",
    "        cvscores2.append(f12)\n",
    "        \n",
    "    \n",
    "    results.append({\"parameters\": param, \"f1\": np.mean(cvscores2), \"f1-noise-on\": np.mean(cvscores), \"std\": np.std(cvscores)})\n",
    "    \n",
    "results.sort(key=lambda x: x['f1'])\n",
    "with open('../Data/static_results.pickle', 'wb') as fileObj:\n",
    "    pickle.dump(results, fileObj)  \n",
    "\n",
    "print(\"DONE AND SAVED SORTED RESULTS\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DYNAMIC MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===Dynamic - Fold  0  - Param set number  0  ======\n",
      "===Dynamic - Fold  1  - Param set number  0  ======\n",
      "===Dynamic - Fold  2  - Param set number  0  ======\n",
      "===Dynamic - Fold  0  - Param set number  1  ======\n",
      "===Dynamic - Fold  1  - Param set number  1  ======\n",
      "===Dynamic - Fold  2  - Param set number  1  ======\n",
      "DONE AND SAVED SORTED RESULTS\n"
     ]
    }
   ],
   "source": [
    "# Testing dynamic model without noise model attached\n",
    "results = []\n",
    "best_score = 0\n",
    "for h, param in enumerate(dynamic_params[:40]):\n",
    "    cvscores = []\n",
    "    cvscores2 = []\n",
    "    for j, (train_idx, val_idx) in enumerate(folds[0:3]):\n",
    "        print(\"===Dynamic - Fold \",j,\" - Param set number \",h,\" ======\")\n",
    "        \n",
    "        x_train_cv = x_train[train_idx]\n",
    "        y_train_cv = y_train[train_idx]\n",
    "        x_valid_cv = x_train[val_idx]\n",
    "        y_valid_cv= y_train[val_idx]\n",
    "        \n",
    "        model = build_model(param)\n",
    "        model.fit(x_train_cv, y_train_cv,\n",
    "                  batch_size=batch_size,\n",
    "                  epochs=epochs,\n",
    "                  verbose=0,\n",
    "                  validation_data=(x_valid_cv, y_valid_cv),\n",
    "                  callbacks=[print_callback])\n",
    "        \n",
    "        pred = model.predict(x_test, verbose=0)\n",
    "        f1 = f1_score([max(enumerate(i), key=operator.itemgetter(1))[0] for i in pred.tolist()], [max(enumerate(i), key=operator.itemgetter(1))[0] for i in y_test.tolist()], average='micro')\n",
    "        cvscores.append(f1) \n",
    "        \n",
    "        #remove the noise layers to reveal what the base model learnt\n",
    "        model2 = Model(model.input, model.layers[-4].output)\n",
    "        pred2 = model2.predict(x_test, verbose=0)\n",
    "        f12 = f1_score([max(enumerate(i), key=operator.itemgetter(1))[0] for i in pred2.tolist()], [max(enumerate(i), key=operator.itemgetter(1))[0] for i in y_test.tolist()], average='micro')\n",
    "        cvscores2.append(f12)\n",
    "        if (f12 > best_score):\n",
    "            pred_saved = pred2\n",
    "        \n",
    "    \n",
    "    results.append({\"parameters\": param, \"f1\": np.mean(cvscores2), \"f1-noise-on\": np.mean(cvscores), \"std\": np.std(cvscores)})\n",
    "     \n",
    "results.sort(key=lambda x: x['f1'])\n",
    "with open('../Data/dynamic_results.pickle', 'wb') as fileObj:\n",
    "    pickle.dump(results, fileObj)  \n",
    "    \n",
    "with open('../Data/dynamic_pred.pickle', 'wb') as fileObj:\n",
    "    pickle.dump(pred_saved, fileObj) \n",
    "\n",
    "print(\"DONE AND SAVED SORTED RESULTS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
