{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "from time import time\n",
    "from scipy.stats import randint as sp_randint\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joins the features extracted from the pics using Openface and the gender/ethnicity label table given by James. Then we do a test training on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "OF_labels = pd.read_csv(\"./OF_featurelist/labels.csv\", header=None, names=[\"drop\", \"id\"])\n",
    "OF_features = pd.read_csv(\"./OF_featurelist/reps.csv\", header=None)\n",
    "result = pd.concat([OF_labels, OF_features], axis=1)\n",
    "result[\"id\"] = result[\"id\"].apply(lambda x: x.split(\"/\")[-1][:-4]) \n",
    "result['id'] = result['id'].apply(pd.to_numeric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "eth_df = pd.read_csv(\"../Images/GenderEthnicityResources/binned_id_gender_ethnicity.csv\")\n",
    "joined = pd.merge(eth_df, result, how='right', on=\"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dropping  881 NaNs\n"
     ]
    }
   ],
   "source": [
    "print(\"dropping \", joined[\"gender\"].isnull().sum(), \"NaNs\")\n",
    "joined =  joined.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do the same for the Feret dataset features and make it the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "OF_labels = pd.read_csv(\"./FERET_Dataset/labels.csv\", header=None, names=[\"drop\", \"id\"])\n",
    "OF_features = pd.read_csv(\"./FERET_Dataset/reps.csv\", header=None)\n",
    "result = pd.concat([OF_labels, OF_features], axis=1)\n",
    "feret_table = pd.read_pickle(\"./FERET_Dataset/gender_table\")\n",
    "\n",
    "result[\"id\"] = result[\"id\"].apply(lambda x: x.split(\"/\")[-1][:-4]) \n",
    "feret_table[\"path\"] = feret_table[\"path\"].apply(lambda x: x.split(\"/\")[-1][:-5])\n",
    "feret_table.rename(inplace = True, columns={\"path\": \"id\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "caucasian     5150\n",
       "eastasian     1549\n",
       "african        645\n",
       "southasian     465\n",
       "hispanic       447\n",
       "other          117\n",
       "Name: race, dtype: int64"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "joined2 = pd.merge(feret_table, result, how='right', on=\"id\")\n",
    "joined2['race'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining the two tables, encoding classes and shuffling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      index_x      0  index_y\n",
      "0   caucasian  13567        1\n",
      "1    hispanic   3474        3\n",
      "2   eastasian   2070        2\n",
      "3  southasian   1359        5\n",
      "4     african    711        0\n",
      "5       other    117        4\n"
     ]
    }
   ],
   "source": [
    "together = pd.concat([joined2[\"race\"], joined[\"ethnicity\"]], axis=0)\n",
    "one = pd.DataFrame(together.value_counts()).reset_index()\n",
    "together = together.astype('category').cat.codes\n",
    "two = pd.DataFrame(together.value_counts()).reset_index()\n",
    "print(pd.merge(one, two, how='right', on=0))\n",
    "\n",
    "\n",
    "y2 = together[0:joined2.shape[0]]\n",
    "X2 = joined2.loc[:, \"drop\":].drop(['drop'], axis=1)\n",
    "y = together[joined2.shape[0]:]\n",
    "X = joined.loc[:, \"drop\":].drop(['drop'], axis=1)\n",
    "\n",
    "X_train2,X_test2,y_train2,y_test2 = train_test_split(X2.index,y2,test_size=0.999999999)\n",
    "X_train2 = X2.iloc[X_train2]\n",
    "X_test2 = X2.iloc[X_test2]\n",
    "\n",
    "X_train,X_test,y_train,y_test = train_test_split(X.index,y,test_size=0.0000000001)\n",
    "X_train = X.iloc[X_train]\n",
    "X_test = X.iloc[X_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train random forest using grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomizedSearchCV took 398.45 seconds for 20 candidates parameter settings.\n",
      "Model with rank: 1\n",
      "Mean validation score: 0.659 (std: 0.003)\n",
      "Parameters: {'bootstrap': False, 'criterion': 'entropy', 'max_depth': None, 'max_features': 5, 'min_samples_leaf': 9, 'min_samples_split': 7}\n",
      "\n",
      "Model with rank: 2\n",
      "Mean validation score: 0.659 (std: 0.005)\n",
      "Parameters: {'bootstrap': False, 'criterion': 'entropy', 'max_depth': None, 'max_features': 10, 'min_samples_leaf': 5, 'min_samples_split': 6}\n",
      "\n",
      "Model with rank: 3\n",
      "Mean validation score: 0.658 (std: 0.005)\n",
      "Parameters: {'bootstrap': False, 'criterion': 'gini', 'max_depth': None, 'max_features': 5, 'min_samples_leaf': 2, 'min_samples_split': 5}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier(n_estimators=40)\n",
    "# Utility function to report best scores\n",
    "def report(results, n_top=3):\n",
    "    for i in range(1, n_top + 1):\n",
    "        candidates = np.flatnonzero(results['rank_test_score'] == i)\n",
    "        for candidate in candidates:\n",
    "            print(\"Model with rank: {0}\".format(i))\n",
    "            print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n",
    "                  results['mean_test_score'][candidate],\n",
    "                  results['std_test_score'][candidate]))\n",
    "            print(\"Parameters: {0}\".format(results['params'][candidate]))\n",
    "            print(\"\")\n",
    "            \n",
    "param_dist = {\"max_depth\": [3, None],\n",
    "              \"max_features\": sp_randint(1, 11),\n",
    "              \"min_samples_split\": sp_randint(2, 11),\n",
    "              \"min_samples_leaf\": sp_randint(1, 11),\n",
    "              \"bootstrap\": [True, False],\n",
    "              \"criterion\": [\"gini\", \"entropy\"]}\n",
    "\n",
    "# run randomized search\n",
    "n_iter_search = 20\n",
    "random_search = RandomizedSearchCV(clf, param_distributions=param_dist,\n",
    "                                   n_iter=n_iter_search)\n",
    "\n",
    "start = time()\n",
    "random_search.fit(X_train, y_train)\n",
    "print(\"RandomizedSearchCV took %.2f seconds for %d candidates\"\n",
    "      \" parameter settings.\" % ((time() - start), n_iter_search))\n",
    "report(random_search.cv_results_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[   0  642    1    1    1    0]\n",
      " [   0 5141    2    6    1    0]\n",
      " [   0  969  543   12   25    0]\n",
      " [   0  416    2   22    7    0]\n",
      " [   0  104   10    3    0    0]\n",
      " [   0  423    0    6   36    0]]\n",
      "0.234070511585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/gc1569/anaconda3/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions = random_search.predict(X_test2)\n",
    "print(confusion_matrix(y_test2, predictions))\n",
    "print(f1_score(y_test2, predictions, average='macro'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics.cluster import v_measure_score\n",
    "print(\"%.6f\" % v_measure_score([0, 0, 1, 1], [5, 5, 2, 2]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
